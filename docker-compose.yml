services:
  # FastAPI application
  api:
    build:
      context: .
      dockerfile: ./src/api/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - LITELLM_URL=http://litellm:8000
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      - litellm
      - mlflow
    volumes:
      - .:/app
      - ./mlflow-data:/mlflow-data
    working_dir: /app
    networks:
      - llmops-network
    command: ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # LiteLLM proxy
  litellm:
    build:
      context: .
      dockerfile: ./litellm/Dockerfile
    ports:
      - "8001:8000"
    environment:
      # API Keys for the different models
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      # MLflow configuration for native tracing
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
    # OR (the one below for information): - ./litellm/litellm-config.yaml:/app/config.yaml
      - ./litellm/litellm-config-security.yaml:/app/config.yaml
    command: ["--config", "/app/config.yaml", "--port", "8000"]
    networks:
      - llmops-network
    depends_on:
      - mlflow
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MLflow for prompt tracing
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5001:5000"
    volumes:
      - ./mlflow-data:/mlflow-data
    networks:
      - llmops-network
    command: >
      mlflow server 
      --host 0.0.0.0 
      --port 5000 
      --backend-store-uri sqlite:///mlflow-data/mlflow.db 
      --default-artifact-root file:///mlflow-data/mlartifacts 
      --serve-artifacts

networks:
  llmops-network:
    driver: bridge