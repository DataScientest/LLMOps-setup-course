# LiteLLM Proxy Configuration - Correct Fallback Setup

# Define each model with a unique name
model_list:
  - model_name: gpt-4o-primary
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gemini-secondary
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      api_key: os.environ/GEMINI_API_KEY
  - model_name: openrouter-fallback
    litellm_params:
      model: openrouter/mistralai/mistral-7b-instruct:free
      api_key: os.environ/OPENROUTER_API_KEY
  # Define the smart-router alias that uses fallbacks
  - model_name: smart-router
    litellm_params:
      model: gpt-4o
      api_key: os.environ/OPENAI_API_KEY

# Configure fallback mapping
router_settings:
  fallbacks: 
    - "smart-router": ["gemini-secondary", "openrouter-fallback"]

# 3. General settings for LiteLLM.
litellm_settings:
  log_level: DEBUG

# 4. Callback settings for native integrations.
callback_settings:
  mlflow: True
